{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bird Sound Classification\n",
    "## by ABBOUDI Mohammed Amine\n",
    "### 28 october 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description of the project:\n",
    "The goal is to predict the set of bird species that are present given a ten-second audio clip. This is a multi- label supervised classification problem. The training data consists of audio recordings paired with the set of species that are present.\n",
    "\n",
    "We have 645 audio recordings, of which 322 are for training and 323 for testing.\n",
    "\n",
    "We will start by computing the corresponding spectrograms, process them so as to extract the relevent data.\n",
    "Spectrogram Images Precessing Pipeline:\n",
    "    * Gaussian Filtering for smoothing and noise reduction\n",
    "    * Applying a local gradient to make the bird chirps \"pop out\"\n",
    "    * Using a binary filter to choose only the most energy dense bits of audio\n",
    "    * Applying binary closing to a binary opening version of the image as a good method for noise reduction\n",
    "    * Filling the hollow remaining feature\n",
    "    * and removing small stains along a threshold of 100 frames.\n",
    "\n",
    "After processing all spectrograms, we move on to syllable segmentation or feature extraction using simple framing and extraction techniques. We then dispose of the original files and only keep the segments and label them with each bird species.\n",
    "\n",
    "Template matching is used to extract the patterns from each observation.\n",
    "\n",
    "We then construct final training and testing dataframes, which consist of the patterns extracted, the regional clusters and the histogram off segments provided in the supplemental data.\n",
    "\n",
    "Training the models, since 19 models are generated each fold to predict each bird species' probability to be present in each test recording. This computationally costly, so we restrict the model to the 100 most important features which are computed for each bird species, since they have different patters and therefore different significant features.\n",
    "\n",
    "The probabilities are averaged across all folds of cross-validation to have a more robust model."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Importing required librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import wave\n",
    "from scipy import fft\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.filters import rank\n",
    "from skimage.morphology import remove_small_objects\n",
    "from skimage.morphology import  disk\n",
    "from skimage.feature import match_template\n",
    "from sklearn.feature_selection import SelectKBest,f_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Directories for quick access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essential = 'mlsp_contest_dataset/essential_data/'\n",
    "supplemental = 'supplemental_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WAV file reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = pd.read_csv(essential+'rec_id2filename.txt')\n",
    "\n",
    "def read_wav(filename):\n",
    "    file = wave.open(filename,'r') # Init of Wave_read Object\n",
    "    data = file.readframes(file.getnframes()) # returns data in hex text\n",
    "    data = np.frombuffer(buffer = data, dtype = 'short') # short is the only dtype that outputs 160000 frames, the data must have been encoded with it\n",
    "    file.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "read_wav(essential+'src_wavs/'+filenames['filename'][1]+'.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading wav files and computing spectrograms (HANNING WINDOW)\n",
    "spectros = []\n",
    "sr = 512 # sampling rate of each window\n",
    "hann_window =  0.5*(1 -np.cos(np.array(range(sr))*2*np.pi/(sr-1) )) # Generating Hanning Windows\n",
    "fr = 16000 # Framerate\n",
    "length = 10 # Length of each audio clip in secs\n",
    "step = 4 # window sliding step\n",
    "\n",
    "for index in range(filenames.shape[0]):\n",
    "    fft_i = []\n",
    "    wav = read_wav(essential+'src_wavs/'+filenames.iloc[index,1]+'.wav')\n",
    "    for j in range(int(step*length*fr/sr)-step):\n",
    "        vec = wav[int(j * sr/step) : int((j+step) * sr/step)] * hann_window\n",
    "        fft_i.append(abs(fft(vec,sr)[:int(sr/2)]))\n",
    "    \n",
    "    spectros.append(np.array(fft_i))\n",
    "spectros[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = pd.read_csv(essential+'CVfolds_2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rec_label = pd.read_csv(essential+'rec_labels_test_hidden.txt', sep=';')\n",
    "labels = pd.DataFrame(np.zeros((rec_label.shape[0], 19)))\n",
    "for i in range(rec_label.shape[0]):\n",
    "    splits = rec_label.iloc[i][0].split(',')\n",
    "    del splits[0]\n",
    "    for j in splits:\n",
    "        if (j!='?'):\n",
    "            labels.iloc[i,int(j)] = 1\n",
    "labels = labels.join(folds)\n",
    "labels['filename'] = filenames.filename # useful labels dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.transpose(spectros[20])\n",
    "\n",
    "# Deleting the lower as they correspond mainly to background noise, and bird chirps are on the higher end of the spectrum\n",
    "img_crop = img[56:,:]\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Spectrogram')\n",
    "plt.imshow(img_crop)\n",
    "# a log version of the spectrogram appears to show much more detail, we will use both for feature extraction\n",
    "img_log = np.log10(img_crop + 1e-3)\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Logarithm Spectrogram')\n",
    "plt.imshow(img_log,cmap=plt.cm.afmhot_r)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Spectrograms and Extracting features \n",
    "\n",
    "for index in range(labels.shape[0]):\n",
    "    isone = labels.iloc[index,0:19].sum() #using only recording where one and only one bird sings\n",
    "    istest = labels.iloc[index,20]\n",
    "    \n",
    "    if (isone ==1 and istest ==0): #using only recording where one and only one bird sings from the training set\n",
    "        img = np.transpose(spectros[index])\n",
    "        # Deleting the lower as they correspond mainly to background noise, and bird chirps are on the higher end of the spectrum\n",
    "        img_crop = img[56:,:]\n",
    "        #Applying a Gaussian Filter on the spectrograms\n",
    "        img_gauss = sp.ndimage.gaussian_filter(img_crop, sigma=3)\n",
    "        #Local Gradient\n",
    "        img_gauss_gradient = rank.gradient(img_as_ubyte((img_gauss-np.min(img_gauss) ) /(np.max(img_gauss - np.min(img_gauss))))\n",
    "                                           , disk(3))\n",
    "        #Applying a Binary Filter\n",
    "        img_gauss_gradient_binary = img_gauss_gradient > np.percentile(img_gauss_gradient,90) # We keep only the most \"energetic\" values i.e. in the 90th percentile\n",
    "        #Binary Closing\n",
    "        img_gauss_gradient_binary_closing = sp.ndimage.binary_closing(sp.ndimage.binary_opening(img_gauss_gradient_binary))\n",
    "        img_fin = sp.ndimage.binary_fill_holes(img_gauss_gradient_binary_closing)\n",
    "        img_fin = remove_small_objects(img_fin,50)\n",
    "        \n",
    "        # a log version of the spectrogram appears to show much more detail, we will use both for feature extraction\n",
    "        img_log = np.log10(img_crop + 1e-5)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))   \n",
    "plt.imshow(img_fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syllable Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Spectrograms and Extracting features \n",
    "segmented_feautures = []\n",
    "\n",
    "for index in range(labels.shape[0]):\n",
    "    istest = labels.iloc[index,20] #Check if observation belongs to the test set\n",
    "    isone = labels.iloc[index,0:19].sum() #using only recording where one and only one bird sings\n",
    "\n",
    "    \n",
    "    if (isone ==1 and istest ==0): #using only recording where one and only one bird sings from the training set\n",
    "        img = np.transpose(spectros[index])\n",
    "        # Deleting the lower as they correspond mainly to background noise, and bird chirps are on the higher end of the spectrum\n",
    "        img_crop = img[56:,:]\n",
    "        #Applying a Gaussian Filter on the spectrograms\n",
    "        img_gauss = sp.ndimage.gaussian_filter(img_crop, sigma=3)\n",
    "        #Local Gradient\n",
    "        img_gauss_gradient = rank.gradient(img_as_ubyte((img_gauss-np.min(img_gauss) ) /(np.max(img_gauss - np.min(img_gauss))))\n",
    "                                           , disk(3))\n",
    "        #Applying a Binary Filter\n",
    "        img_gauss_gradient_binary = img_gauss_gradient > np.percentile(img_gauss_gradient,90) # We keep only the most \"energetic\" values i.e. in the 90th percentile\n",
    "        #Binary Closing\n",
    "        img_gauss_gradient_binary_closing = sp.ndimage.binary_closing(sp.ndimage.binary_opening(img_gauss_gradient_binary))\n",
    "        img_fin = sp.ndimage.binary_fill_holes(img_gauss_gradient_binary_closing)\n",
    "        img_fin = remove_small_objects(img_fin,99)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        label, num_features = sp.ndimage.label(img_fin)\n",
    "\n",
    "        for id_feature in range(num_features):\n",
    "            # Calculating the pixel coordinates of each feature\n",
    "            feature = (label == (id_feature+1))\n",
    "            feature = feature*1 # Changing True and False to 1 and 0\n",
    "            ft_x = feature.max(axis =  0)\n",
    "            ft_x_max = np.max(ft_x*range(len(ft_x)))\n",
    "            ft_x[ft_x==0] = 2\n",
    "            ft_x_min = np.argmin(ft_x)\n",
    "            \n",
    "            ft_y = feature.max(axis =  1)\n",
    "            ft_y_max = np.max(ft_y*range(len(ft_y)))\n",
    "            ft_y[ft_y==0] = 2\n",
    "            ft_y_min = np.argmin(ft_y)\n",
    "            \n",
    "            # Cropping the spectrogram image to extract the final image of each feature (pattern)\n",
    "            frame = [ft_y_min, ft_y_max, ft_x_min, ft_x_max]\n",
    "            feature_img = img_gauss[ft_y_min:ft_y_max,ft_x_min:ft_x_max]\n",
    "            segmented_feautures.append([index, id_feature, frame, feature_img])\n",
    "            \n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.subplot(5,1,1)\n",
    "plt.title('After Local Gradient')\n",
    "plt.imshow(img_gauss_gradient)\n",
    "plt.subplot(5,1,2)\n",
    "plt.title('After Binary Filter')\n",
    "plt.imshow(img_gauss_gradient_binary)\n",
    "plt.subplot(5,1,3)\n",
    "plt.title('After Binary Closing')\n",
    "plt.imshow(img_gauss_gradient_binary_closing)\n",
    "plt.subplot(5,1,4)\n",
    "plt.title('After Filling Holes')\n",
    "plt.imshow(img_fin)\n",
    "plt.subplot(5,1,5)\n",
    "plt.title('After Labeling')\n",
    "plt.imshow(label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Spectrograms and Extracting features \n",
    "segmented_feautures_log = []\n",
    "\n",
    "for index in range(labels.shape[0]):\n",
    "    istest = labels.iloc[index,20] #Check if observation belongs to the test set\n",
    "    isone = labels.iloc[index,0:19].sum() #using only recording where one and only one bird sings\n",
    "\n",
    "    \n",
    "    if (isone ==1 and istest ==0): #using only recording where one and only one bird sings from the training set\n",
    "        img = np.transpose(spectros[index])\n",
    "        \n",
    "        # Deleting the lower as they correspond mainly to background noise, and bird chirps are on the higher end of the spectrum\n",
    "        img_crop = img[56:,:]\n",
    "        \n",
    "        #Applying the Log Function\n",
    "        img_log = np.log10(img_crop + 1e-5)\n",
    "        \n",
    "        #Applying a Gaussian Filter on the spectrograms\n",
    "        img_log_gauss = sp.ndimage.gaussian_filter(img_log, sigma=3)\n",
    "        \n",
    "        #Local Gradient\n",
    "        img_log_gauss_gradient = rank.gradient(img_as_ubyte((img_log_gauss-np.min(img_log_gauss) ) /(np.max(img_log_gauss - np.min(img_log_gauss))))\n",
    "                                           , disk(3)) \n",
    "                                # Makes areas of interest (which are maxima of amplitude and/or frequency more visible)\n",
    "        \n",
    "        #Applying a Binary Filter\n",
    "        img_log_gauss_gradient_binary = (img_log_gauss_gradient > np.percentile(img_log_gauss_gradient,90)) # We keep only the most \"energetic\" values i.e. in the 90th percentile\n",
    "        \n",
    "        #Binary Closing\n",
    "        img_log_gauss_gradient_binary_closing = sp.ndimage.binary_closing(sp.ndimage.binary_opening(img_log_gauss_gradient_binary))\n",
    "        \n",
    "        # Filling the holes present in the Spectrograms\n",
    "        img_log_fin = sp.ndimage.binary_fill_holes(img_log_gauss_gradient_binary_closing)\n",
    "        \n",
    "        # Removing the small 'stains' still present\n",
    "        img_log_fin = remove_small_objects(img_log_fin,100) \n",
    "        # We set the minimal threshold size of each segment to 100 frames\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        label, num_features = sp.ndimage.label(img_log_fin)\n",
    "\n",
    "        for id_feature in range(num_features):\n",
    "            # Calculating the pixel coordinates of each feature\n",
    "            feature = (label == (id_feature+1))\n",
    "            feature = feature*1 # Changing True and False to 1 and 0\n",
    "            ft_x = feature.max(axis =  0)\n",
    "            ft_x_max = np.max(ft_x*range(len(ft_x)))\n",
    "            ft_x[ft_x==0] = 2\n",
    "            ft_x_min = np.argmin(ft_x)\n",
    "            \n",
    "            ft_y = feature.max(axis =  1)\n",
    "            ft_y_max = np.max(ft_y*range(len(ft_y)))\n",
    "            ft_y[ft_y==0] = 2\n",
    "            ft_y_min = np.argmin(ft_y)\n",
    "            \n",
    "            # Cropping the spectrogram image to extract the final image of each feature (pattern)\n",
    "            frame = [ft_y_min, ft_y_max, ft_x_min, ft_x_max]\n",
    "            feature_img = img_gauss[ft_y_min:ft_y_max,ft_x_min:ft_x_max]\n",
    "            segmented_feautures_log.append([index, id_feature, frame, feature_img])\n",
    "            \n",
    "plt.figure(figsize=(20, 15))\n",
    "plt.subplot(5,1,1)\n",
    "plt.title('After Local Gradient')\n",
    "plt.imshow(img_log_gauss_gradient)\n",
    "plt.subplot(5,1,2)\n",
    "plt.title('After Binary Filter')\n",
    "plt.imshow(img_log_gauss_gradient_binary)\n",
    "plt.subplot(5,1,3)\n",
    "plt.title('After Binary Closing')\n",
    "plt.imshow(img_log_gauss_gradient_binary_closing)\n",
    "plt.subplot(5,1,4)\n",
    "plt.title('After Filling Holes')\n",
    "plt.imshow(img_log_fin)\n",
    "plt.subplot(5,1,5)\n",
    "plt.title('After Labeling')\n",
    "plt.imshow(label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can notice between the two that there is much less noise in the logarithmic image compared to the normal one on the higher end of the frequency spectrum, while the is more noise on the log image in the the lower end.\n",
    "Which makes the choise of using both justifiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template Matching"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Disclaimer: \n",
    "    I unintentionally hit the restart button, so all variables and outputs were erased.\n",
    "    The code below takes more than 12 hours to complete!\n",
    "    But the results are documented in the last cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SKIP IF TRAIN AND TEST FEATURES ARE PRECOMPUTED\n",
    "\n",
    "train_features = []\n",
    "train_log_features = []\n",
    "test_features = []\n",
    "test_log_features = []\n",
    "\n",
    "for index in range(len(labels)):\n",
    "    \n",
    "    if (index%161 ==0):\n",
    "        print('{}% completed..'.format(int(index/len(labels))))\n",
    "    \n",
    "    istest = labels.iloc[index,20]\n",
    "\n",
    "    if(istest == 0):\n",
    "        img = np.transpose(spectros[index])\n",
    "        \n",
    "        img_crop = img[56:,:] # Focus on the Relevant Frequency Domain\n",
    "        \n",
    "        img_gauss = sp.ndimage.gaussian_filter(img_crop, sigma=3) # Gaussian filter\n",
    "\n",
    "        #Applying the Log Function\n",
    "        img_log = np.log10(img_crop + 1e-5)\n",
    "        \n",
    "        img_log_gauss = sp.ndimage.gaussian_filter(img_log, sigma=3) # Gaussian filter\n",
    "        \n",
    "        all_segments = []\n",
    "        for x in range(len(segmented_feautures)):\n",
    "            segment = segmented_feautures [x][3]    # Getting the segmented feature\n",
    "            f_min   = segmented_feautures [x][2][0] # Getting the minimal frequency of the feature\n",
    "            f_max   = segmented_feautures [x][2][1] # Getting the maximal frequency of the feature\n",
    "            \n",
    "            # Making sure index isn't out of range\n",
    "            if(f_min > 5):\n",
    "                f_min = f_min-5\n",
    "            else:\n",
    "                f_min = 0\n",
    "            \n",
    "            if(f_max < 194):\n",
    "                f_max = f_max+5\n",
    "            else:\n",
    "                f_max = 199\n",
    "            \n",
    "            spectrogram_part = img_gauss[f_min:f_max,:]\n",
    "            # Extracting the relevant frequency interval spanning the entire X-axis of the image\n",
    "            matched = match_template(spectrogram_part, segment)\n",
    "            all_segments.append(np.max(matched))\n",
    "            \n",
    "        train_features.append(all_segments)\n",
    "            \n",
    "            \n",
    "        all_segments = []\n",
    "        for x in range(len(segmented_feautures)):\n",
    "            segment = segmented_feautures [x][3]    # Getting the segmented feature\n",
    "            f_min   = segmented_feautures [x][2][0] # Getting the minimal frequency of the feature\n",
    "            f_max   = segmented_feautures [x][2][1] # Getting the maximal frequency of the feature\n",
    "            \n",
    "            # Making sure index isn't out of range\n",
    "            if(f_min > 5):\n",
    "                f_min = f_min-5\n",
    "            else:\n",
    "                f_min = 0\n",
    "            \n",
    "            if(f_max < 194):\n",
    "                f_max = f_max+5\n",
    "            else:\n",
    "                f_max = 199\n",
    "            \n",
    "            spectrogram_part = img_gauss[f_min:f_max,:]\n",
    "            # Extracting the relevant frequency interval spanning the entire X-axis of the image\n",
    "            matched = match_template(spectrogram_part, segment)\n",
    "            all_segments.append(np.max(matched))\n",
    "        \n",
    "        train_log_features.append(all_segments)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        \n",
    "        img = np.transpose(spectros[index])\n",
    "        \n",
    "        img_crop = img[56:,:] # Focus on the Relevant Frequency Domain\n",
    "        \n",
    "        img_gauss = sp.ndimage.gaussian_filter(img_crop, sigma=3) # Gaussian filter\n",
    "\n",
    "        #Applying the Log Function\n",
    "        img_log = np.log10(img_crop + 1e-5)\n",
    "        \n",
    "        img_log_gauss = sp.ndimage.gaussian_filter(img_log, sigma=3) # Gaussian filter\n",
    "        \n",
    "        all_segments = []\n",
    "        for x in range(len(segmented_feautures)):\n",
    "            segment = segmented_feautures [x][3]    # Getting the segmented feature\n",
    "            f_min   = segmented_feautures [x][2][0] # Getting the minimal frequency of the feature\n",
    "            f_max   = segmented_feautures [x][2][1] # Getting the maximal frequency of the feature\n",
    "            \n",
    "            # Making sure index isn't out of range\n",
    "            if(f_min > 5):\n",
    "                f_min = f_min - 5\n",
    "            else:\n",
    "                f_min = 0\n",
    "            \n",
    "            if(f_max < 194):\n",
    "                f_max = f_max + 5\n",
    "            else:\n",
    "                f_max = 199\n",
    "            \n",
    "            spectrogram_part = img_gauss[f_min:f_max,:]\n",
    "            # Extracting the relevant frequency interval spanning the entire X-axis of the image\n",
    "            matched = match_template(spectrogram_part, segment)\n",
    "            all_segments.append(np.max(matched))\n",
    "            \n",
    "        test_features.append(all_segments)\n",
    "            \n",
    "            \n",
    "        all_segments = []\n",
    "        for x in range(len(segmented_feautures)):\n",
    "            segment = segmented_feautures [x][3]    # Getting the segmented feature\n",
    "            f_min   = segmented_feautures [x][2][0] # Getting the minimal frequency of the feature\n",
    "            f_max   = segmented_feautures [x][2][1] # Getting the maximal frequency of the feature\n",
    "            \n",
    "            # Making sure index isn't out of range\n",
    "            if(f_min > 5):\n",
    "                f_min = f_min - 5\n",
    "            else:\n",
    "                f_min = 0\n",
    "            \n",
    "            if(f_max < 194):\n",
    "                f_max = f_max + 5\n",
    "            else:\n",
    "                f_max = 199\n",
    "            \n",
    "            spectrogram_part = img_gauss[f_min:f_max,:] \n",
    "            # Extracting the relevant frequency interval spanning the entire X-axis of the image\n",
    "            matched = match_template(spectrogram_part, segment)\n",
    "            all_segments.append(np.max(matched))\n",
    "        \n",
    "        test_log_features.append(all_segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SKIP IF TRAIN AND TEST FEATURES ARE PRECOMPUTED\n",
    "\n",
    "# Backup of Features\n",
    "\n",
    "train_features1 = train_features\n",
    "train_log_features1 = train_log_features\n",
    "test_features1 = test_features\n",
    "test_log_features1 = test_log_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SKIP IF TRAIN AND TEST FEATURES ARE PRECOMPUTED\n",
    "\n",
    "# Collecting the complete training dataframe\n",
    "# We will be using some of the supplemental data provided, namely the histogram of segments.\n",
    "\n",
    "hist_segmts =  pd.read_csv(supplemental + 'histogram_of_segments.txt', sep = ',', names = ['rec_id']+['hist_'+ str(x) for x in range(100)] ,skiprows=1, header=0)\n",
    "\n",
    "df = pd.merge(left = labels, right = hist_segmts , left_on = 'rec_id', right_on = 'rec_id', how = 'left').fillna(0)\n",
    "\n",
    "\n",
    "train_features = pd.DataFrame(train_features, columns=['feature_'+str(x) for x in range(np.array(train_features).shape[1])])\n",
    "train_features['rec_id']= df[df.fold==0].index\n",
    "\n",
    "train_log_features = pd.DataFrame(train_log_features, columns=['feature_log_'+str(x) for x in range(np.array(train_log_features).shape[1])])\n",
    "train_log_features['rec_id']= df[df.fold==0].index\n",
    "\n",
    "test_features = pd.DataFrame(test_features, columns=['feature_'+str(x) for x in range(np.array(train_features).shape[1])])\n",
    "test_features['rec_id']= df[df.fold==1].index\n",
    "\n",
    "test_log_features = pd.DataFrame(test_log_features, columns=['feature_log_'+str(x) for x in range(np.array(train_log_features).shape[1])])\n",
    "test_log_features['rec_id']= df[df.fold==1].index\n",
    "\n",
    "all_train_features = pd.merge(left = train_features, right = train_log_features , left_on = 'rec_id', right_on = 'rec_id')\n",
    "all_test_features  = pd.merge(left = test_features, right = test_log_features , left_on = 'rec_id', right_on = 'rec_id')\n",
    "\n",
    "# Adding regional clustering data\n",
    "region = []\n",
    "\n",
    "for i  in range(len(df)):\n",
    "    sub = df.filename.iloc[i][:7]\n",
    "    sub = sub[2:sub.find('_')]\n",
    "    region.append(int(sub))\n",
    "    \n",
    "df['Region'] = region\n",
    "\n",
    "train = pd.merge(left = df[df.fold==0], right = all_train_features , left_index = True, right_on = 'rec_id')\n",
    "test = pd.merge(left = df[df.fold==1], right = all_test_features , left_index = True, right_on = 'rec_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell if csv files of the training and testing set are available.\n",
    "\n",
    "train = pd.read_csv(supplemental+'train.csv',sep=',')\n",
    "test = pd.read_csv(supplemental+'test.csv',sep=',')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['feature_'+ str(x) for x in range(322)] + ['feature_log_' + str(x) for x in range(313)] + ['hist_'+ str(x) for x in range(100)] + ['Region']\n",
    "\n",
    "\n",
    "n_folds = 15\n",
    "rnd = np.random.randint(0,n_folds,len(train))\n",
    "\n",
    "# Using personalized code for cross_validation since we have a multi-label multi-instance problem.\n",
    "train['cross_val'] = rnd\n",
    "\n",
    "truth = []\n",
    "prediction = []\n",
    "proba = []\n",
    "for i in range(19):\n",
    "    proba.append(np.zeros(len(test)))\n",
    "\n",
    "\n",
    "    \n",
    "for n in range(n_folds):\n",
    "    \n",
    "    train_large = train[train.cross_val != n]\n",
    "    X_train = train_large[cols]\n",
    "    \n",
    "    train_small = train[train.cross_val == n]\n",
    "    X_test = train_small[cols]\n",
    "    \n",
    "    X_proba = test[cols]\n",
    "    \n",
    "    # Loop for generating 19 models\n",
    "    for i in range(19):\n",
    "        \n",
    "        y_train = train_large[str(i)] # column specific to bird i in training\n",
    "        y_test = train_small[str(i)] # column specific to bird i in testing\n",
    "        \n",
    "        best_100 = SelectKBest(f_regression,50)\n",
    "        best_100.fit(X_train, y_train)\n",
    "        X_train_100 = best_100.transform(X_train)\n",
    "        X_test_100 = best_100.transform(X_test)\n",
    "        X_proba_100 = best_100.transform(X_proba)\n",
    "        \n",
    "        #I tried all regressors, but the random forest performed best, followed closely by SVM regressor. \n",
    "        #I opted for RF because it allows more intuitive customizability\n",
    "        \n",
    "        RF = RandomForestRegressor(n_estimators = 500, max_features = 6)# using the Regressor since we are predicting a probability\n",
    "        RF.fit(X_train_100,y_train)\n",
    "        y_pred = RF.predict(X_test_100)\n",
    "        y_proba = RF.predict(X_proba_100)\n",
    "\n",
    "        prediction = prediction + list(y_pred)\n",
    "        truth = truth + list(y_test)\n",
    "        \n",
    "        proba[i] = proba[i] + (y_proba/n_folds) # Averaging across all cross_validation folds\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(truth, prediction, pos_label=1)\n",
    "AUC = metrics.auc(fpr,tpr)\n",
    "print('The AUC of the Model is: {}'.format(AUC) )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The final AUC of the model is 0.897 and the highest Kaggle score is 92.41% which places at n. 11 on the leaderboard (top 14%).\n",
    "I was not able to optimize model parameters given that the computational time is enormous."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
